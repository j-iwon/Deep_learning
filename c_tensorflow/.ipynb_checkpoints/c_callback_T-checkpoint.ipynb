{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd672646-fdc6-4747-9df1-5ec49dd872a1",
   "metadata": {},
   "source": [
    "### Callback API\n",
    "- 모델이 학습 중에 충돌이 발생하거나 네트워크가 끊기면, 모든 훈련 시간이 낭비될 수 있고,  \n",
    "  과적합을 방지하기 위해 훈련을 중간에 중지해야 할 수도 있다.\n",
    "- 모델이 학습을 시작하면 학습이 완료될 때까지 아무런 제어를 하지 못하게 되고,  \n",
    "  신경망 훈련을 완료하는 데에는 몇 시간 또는 며칠이 걸릴 수 있기 때문에 모델을 모니터링하고 제어할 수 있는 기능이 필요하다.\n",
    "- 훈련 시(fit()) Callback API를 등록시키면 반복 내에서 특정 이벤트 발생마다 등록된 callback이 호출되어 수행된다.\n",
    "\n",
    "**ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weight_only=False, mode='auto')**\n",
    "- 특정 조건에 따라서 모델 또는 가중치를 파일로 저장한다.\n",
    "- filepath: \"weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.hdf5\"와 같이 모델의 체크포인트를 저장한다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- save_best_only: 가장 좋은 성능을 나타내는 모델을 저장할 지에 대한 여부\n",
    "- save_weights_only: weights만 저장할 지에 대한 여부\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 좋은 경우를 선택한다.  \n",
    "  *monitor의 성능 지표가 감소해야 좋은 경우 min, 증가해야 좋은 경우 max, monitor의 이름으로부터 자동으로 유추하고 싶다면 auto\n",
    "\n",
    "\n",
    "**ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_lr=0)**\n",
    "- 특정 반복동안 성능이 개선되지 않을 때, 학습률을 동적으로 감소시킨다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- factor: 학습률을 감소시킬 비율, 새로운 학습률 = 기존 학습률 * factor\n",
    "- patience: 학습률을 줄이기 전에 monitor할 반복 횟수\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 좋은 경우를 선택한다.  \n",
    "  *monitor의 성능 지표가 감소해야 좋은 경우 min, 증가해야 좋은 경우 max, monitor의 이름으로부터 자동으로 유추하고 싶다면 auto\n",
    "\n",
    "\n",
    "**EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')**\n",
    "- 특정 반복동안 성능이 개선되지 않을 때, 학습을 조기에 중단한다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- patience: Early Stopping을 적용하기 전에 monitor할 반복 횟수.\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 좋은 경우를 선택한다.  \n",
    "  *monitor의 성능 지표가 감소해야 좋은 경우 min, 증가해야 좋은 경우 max, monitor의 이름으로부터 자동으로 유추하고 싶다면 auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ef30c79-19a0-4f8a-837c-7cf753d0f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "INPUT_SIZE = 28\n",
    "\n",
    "def create_model():\n",
    "    input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))\n",
    "    x = Flatten()(input_tensor)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bde130f-594e-4657-a88c-ccdebee00bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def get_preprocessed_data(images, targets):\n",
    "    images = np.array(images / 255.0, dtype=np.float32)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "def get_preprocessed_ohe(images, targets):\n",
    "    images, targets = get_preprocessed_data(images, targets)\n",
    "    oh_targets = to_categorical(targets)\n",
    "\n",
    "    return images, oh_targets\n",
    "\n",
    "def get_train_valid_test(train_images, train_targets, test_images, test_targets, validation_size=0.2, random_state=124):\n",
    "    train_images, train_oh_targets = get_preprocessed_ohe(train_images, train_targets)\n",
    "    test_images, test_oh_targets = get_preprocessed_ohe(test_images, test_targets)\n",
    "\n",
    "    train_images, validation_images, train_oh_targets, validation_oh_targets = \\\n",
    "    train_test_split(train_images, train_oh_targets, stratify=train_oh_targets, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    return (train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08f5354-1d42-41a3-925a-6f36a952c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28) (48000, 10)\n",
      "(12000, 28, 28) (12000, 10)\n",
      "(10000, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_targets), (test_images, test_targets) = fashion_mnist.load_data()\n",
    "\n",
    "(train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets) = \\\n",
    "get_train_valid_test(train_images, train_targets, test_images, test_targets)\n",
    "\n",
    "print(train_images.shape, train_oh_targets.shape)\n",
    "print(validation_images.shape, validation_oh_targets.shape)\n",
    "print(test_images.shape, test_oh_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f463df61-a4aa-41af-9692-65baa1fc2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D 드라이브의 볼륨: 새 볼륨\n",
      " 볼륨 일련 번호: 02B1-19A2\n",
      "\n",
      " D:\\kdt_0900_hds\\ai\\deep_learning\\c_tensorflow\\callback_files 디렉터리\n",
      "\n",
      "2024-05-27  오후 01:25    <DIR>          .\n",
      "2024-05-27  오후 01:25    <DIR>          ..\n",
      "               0개 파일                   0 바이트\n",
      "               2개 디렉터리  466,331,320,320 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir callback_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff9ffd-4771-41fc-9f0d-a0476a5fe773",
   "metadata": {},
   "source": [
    "### ModelCheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a11f56e-1e2a-42a1-9413-61aa60505104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.7330 - loss: 0.7645 - val_acc: 0.8471 - val_loss: 0.4307\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.8566 - loss: 0.4068 - val_acc: 0.8537 - val_loss: 0.3900\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8665 - loss: 0.3648 - val_acc: 0.8690 - val_loss: 0.3534\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8759 - loss: 0.3375 - val_acc: 0.8721 - val_loss: 0.3516\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8834 - loss: 0.3199 - val_acc: 0.8648 - val_loss: 0.3549\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8908 - loss: 0.2982 - val_acc: 0.8746 - val_loss: 0.3503\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8929 - loss: 0.2886 - val_acc: 0.8745 - val_loss: 0.3356\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8991 - loss: 0.2756 - val_acc: 0.8826 - val_loss: 0.3239\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9051 - loss: 0.2596 - val_acc: 0.8841 - val_loss: 0.3162\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9048 - loss: 0.2572 - val_acc: 0.8844 - val_loss: 0.3136\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9106 - loss: 0.2416 - val_acc: 0.8792 - val_loss: 0.3260\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9109 - loss: 0.2385 - val_acc: 0.8893 - val_loss: 0.3172\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9140 - loss: 0.2311 - val_acc: 0.8852 - val_loss: 0.3202\n",
      "Epoch 14/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9173 - loss: 0.2254 - val_acc: 0.8843 - val_loss: 0.3187\n",
      "Epoch 15/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9185 - loss: 0.2191 - val_acc: 0.8912 - val_loss: 0.3079\n",
      "Epoch 16/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9208 - loss: 0.2123 - val_acc: 0.8882 - val_loss: 0.3231\n",
      "Epoch 17/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9221 - loss: 0.2048 - val_acc: 0.8898 - val_loss: 0.3255\n",
      "Epoch 18/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9275 - loss: 0.1968 - val_acc: 0.8891 - val_loss: 0.3250\n",
      "Epoch 19/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9260 - loss: 0.1936 - val_acc: 0.8937 - val_loss: 0.3261\n",
      "Epoch 20/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9327 - loss: 0.1838 - val_acc: 0.8901 - val_loss: 0.3282\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "    monitor='val_loss',\n",
    "    # 모든 epoch의 파일을 저장하지 않고 좋은 성능이라 판단될 경우만 저장할 때 True설정\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# mcp_cb = ModelCheckpoint(\n",
    "#     filepath=\"./callback_files/model.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.model.keras\",\n",
    "#     monitor='val_loss',\n",
    "#     # 모든 epoch의 파일을 저장하지 않고 좋은 성능이라 판단될 경우만 저장할 때 True설정\n",
    "#     save_best_only=False,\n",
    "#     save_weights_only=False,\n",
    "#     mode='min'\n",
    "# )\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, validation_data=(validation_images, validation_oh_targets), batch_size=64, epochs=20, callbacks=[mcp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bd4edbb-267c-4989-adbf-b98674e7cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D 드라이브의 볼륨: 새 볼륨\n",
      " 볼륨 일련 번호: 02B1-19A2\n",
      "\n",
      " D:\\kdt_0900_hds\\ai\\deep_learning\\c_tensorflow\\callback_files 디렉터리\n",
      "\n",
      "2024-05-28  오전 10:14    <DIR>          .\n",
      "2024-05-28  오전 10:14    <DIR>          ..\n",
      "2024-05-28  오전 10:14           739,600 weights.001-0.4442-0.8094.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.002-0.3692-0.8601.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.003-0.3617-0.8725.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.004-0.3398-0.8802.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.005-0.3336-0.8880.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.008-0.3184-0.9014.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.011-0.3155-0.9090.weights.h5\n",
      "               7개 파일           5,177,200 바이트\n",
      "               2개 디렉터리  466,311,921,664 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir callback_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e194dd8-c6b2-454e-b054-043ed65cbfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.8815 - loss: 0.3695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3670561909675598, 0.8841999769210815]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2506fdd-c490-4f82-9bfc-3c623868d702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.8815 - loss: 0.3695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3670561909675598, 0.8841999769210815]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights('./callback_files/weights.020-0.3282-0.9295.weights.h5')\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "model.evaluate(test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93d6aaa1-0516-4ca6-acce-aa4c754c0f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - acc: 0.8780 - loss: 0.3660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36721712350845337, 0.8792999982833862]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('./callback_files/model.020-0.3350-0.9287.model.keras')\n",
    "model.evaluate(test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c139ef-e649-42cb-a969-7273f4a2f4bf",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85fb1808-7981-4c2f-ad38-ce6c6bbdc049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.7508 - loss: 0.7447 - val_acc: 0.8538 - val_loss: 0.4086 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8565 - loss: 0.4017 - val_acc: 0.8662 - val_loss: 0.3676 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8681 - loss: 0.3579 - val_acc: 0.8717 - val_loss: 0.3539 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8784 - loss: 0.3288 - val_acc: 0.8786 - val_loss: 0.3371 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8866 - loss: 0.3078 - val_acc: 0.8734 - val_loss: 0.3472 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8906 - loss: 0.2964 - val_acc: 0.8763 - val_loss: 0.3350 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.8970 - loss: 0.2747 - val_acc: 0.8605 - val_loss: 0.3810 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9000 - loss: 0.2693 - val_acc: 0.8770 - val_loss: 0.3370 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9105 - loss: 0.2392 - val_acc: 0.8889 - val_loss: 0.3012 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9194 - loss: 0.2182 - val_acc: 0.8906 - val_loss: 0.3006 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9192 - loss: 0.2196 - val_acc: 0.8921 - val_loss: 0.2985 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9221 - loss: 0.2138 - val_acc: 0.8910 - val_loss: 0.2996 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9216 - loss: 0.2141 - val_acc: 0.8912 - val_loss: 0.3012 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9237 - loss: 0.2064 - val_acc: 0.8934 - val_loss: 0.2982 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9250 - loss: 0.2071 - val_acc: 0.8933 - val_loss: 0.2981 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9237 - loss: 0.2107 - val_acc: 0.8931 - val_loss: 0.2984 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9246 - loss: 0.2070 - val_acc: 0.8934 - val_loss: 0.2982 - learning_rate: 1.0000e-06\n",
      "Epoch 18/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - acc: 0.9225 - loss: 0.2106 - val_acc: 0.8935 - val_loss: 0.2982 - learning_rate: 1.0000e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - acc: 0.9230 - loss: 0.2103 - val_acc: 0.8935 - val_loss: 0.2982 - learning_rate: 1.0000e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9241 - loss: 0.2098 - val_acc: 0.8935 - val_loss: 0.2982 - learning_rate: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, validation_data=(validation_images, validation_oh_targets), batch_size=64, epochs=20, callbacks=[rlr_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83170c4-fb33-474a-a260-d52b24d63b95",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27a725f2-daf0-4516-8efa-a630aa3e0bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.7330 - loss: 0.7671 - val_acc: 0.8510 - val_loss: 0.4107\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8579 - loss: 0.3943 - val_acc: 0.8455 - val_loss: 0.4120\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8699 - loss: 0.3576 - val_acc: 0.8638 - val_loss: 0.3645\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8790 - loss: 0.3281 - val_acc: 0.8729 - val_loss: 0.3442\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8861 - loss: 0.3112 - val_acc: 0.8735 - val_loss: 0.3474\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8911 - loss: 0.2933 - val_acc: 0.8785 - val_loss: 0.3274\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8981 - loss: 0.2762 - val_acc: 0.8800 - val_loss: 0.3248\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9015 - loss: 0.2666 - val_acc: 0.8863 - val_loss: 0.3168\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9023 - loss: 0.2618 - val_acc: 0.8826 - val_loss: 0.3251\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9070 - loss: 0.2460 - val_acc: 0.8827 - val_loss: 0.3208\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9078 - loss: 0.2475 - val_acc: 0.8783 - val_loss: 0.3351\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, validation_data=(validation_images, validation_oh_targets), batch_size=64, epochs=20, callbacks=[ely_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaeb9f05-d839-4d2a-b101-73645675b5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.7337 - loss: 0.7724 - val_acc: 0.8519 - val_loss: 0.4254 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8539 - loss: 0.4079 - val_acc: 0.8558 - val_loss: 0.3898 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8694 - loss: 0.3576 - val_acc: 0.8712 - val_loss: 0.3559 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8791 - loss: 0.3296 - val_acc: 0.8661 - val_loss: 0.3596 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8882 - loss: 0.3064 - val_acc: 0.8737 - val_loss: 0.3374 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8923 - loss: 0.2962 - val_acc: 0.8822 - val_loss: 0.3269 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8985 - loss: 0.2729 - val_acc: 0.8852 - val_loss: 0.3187 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9002 - loss: 0.2695 - val_acc: 0.8798 - val_loss: 0.3250 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9008 - loss: 0.2616 - val_acc: 0.8849 - val_loss: 0.3170 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9073 - loss: 0.2461 - val_acc: 0.8808 - val_loss: 0.3217 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9107 - loss: 0.2400 - val_acc: 0.8896 - val_loss: 0.3203 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9245 - loss: 0.2026 - val_acc: 0.8938 - val_loss: 0.2940 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9275 - loss: 0.1999 - val_acc: 0.8947 - val_loss: 0.2911 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9278 - loss: 0.1950 - val_acc: 0.8956 - val_loss: 0.2927 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9298 - loss: 0.1874 - val_acc: 0.8953 - val_loss: 0.2941 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9319 - loss: 0.1844 - val_acc: 0.8953 - val_loss: 0.2922 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "    monitor='val_loss',\n",
    "    # 모든 epoch의 파일을 저장하지 않고 좋은 성능이라 판단될 경우만 저장할 때 True설정\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, validation_data=(validation_images, validation_oh_targets), batch_size=64, epochs=20, callbacks=[mcp_cb, rlr_cb, ely_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
